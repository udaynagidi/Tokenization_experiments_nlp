# -*- coding: utf-8 -*-
"""Tokenization Experiments .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wZG7Ht_nNenWlp7w5KiQJnchJOzlHR98

#Testing tokenizers
"""

from transformers import AutoTokenizer

models = [
    "deepseek-ai/DeepSeek-R1",
    "microsoft/phi-4",
    "NousResearch/Llama-2-7b-chat-hf"]

sentences = [
    "Artificial intelligence is revolutionizing the world.",
    "Supercalifragilisticexpialidocious is a long word!",
    "I love programming in Python."]

for model in models:
    tokenizer = AutoTokenizer.from_pretrained(model)
    print(f"\n--- {model} ---")
    for sentence in sentences:
        tokens = tokenizer(sentence).input_ids
        print(f"Sentence: {sentence}")
        print(f"Token count: {len(tokens)} | Tokens: {tokens}")

word = "vrushali "

for model in models:
    tokenizer = AutoTokenizer.from_pretrained(model)
    tokens = tokenizer(word).input_ids
    decoded_tokens = [tokenizer.decode([t]) for t in tokens]

    print(f"\n--- {model} ---")
    print(f"Tokens: {tokens}")
    print(f"Decoded: {decoded_tokens}")